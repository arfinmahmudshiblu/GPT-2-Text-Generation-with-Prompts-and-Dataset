# GPT-2-Text-Generation-with-Prompts-and-Dataset
# ðŸ“‹Table of Contents
GPT-2 Text Generation

Setting up the Environment

Importing Libraries

Convert the Sentences into the Tokens

Model's Vocab

# Generate the text given the sentence

1. Greedy Search

2. Beam Search

3. Random Sampling

4. Top-K Sampling

5. Top-P Sampling

6. Top-K and Top-P Sampling

# Generate the text with dataset

Download Shakespeare dataset from Huggingface datasets hub

Tokenize all the texts

Split whole dataset into smaller sets of blocks

Initialize Trainer

Cleaning Memory

Perform Training

Evaluate Model

# Generate Samples

1. Greedy Search Text Generation

2. Beam Search Text Generation

3. Random Sampling Text Generation

4. Top-K Sampling Text Generation

5. Top-P Sampling Text Generation

6. Top-K and Top-P Sampling Text Generation

# Text Generation Project Structure
gpt2-text-generation-computer/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ GPT-2 Text Generation.ipynb
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ computers.txt
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained_model/   (model saved here)
â”‚
â””â”€â”€ samples/
    â”œâ”€â”€ greedy_output.txt
    â”œâ”€â”€ beam_output.txt
    â”œâ”€â”€ random_output.txt
    â”œâ”€â”€ topk_output.txt
    â””â”€â”€ topp_output.txt



